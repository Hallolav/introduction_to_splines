{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping wikipedia to find allround ice-skating times\n",
    "\n",
    "In this Notebook, I'll scrape the wikipedia pages containing the results for men's allround world championship results. I'll first start by creating a python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda create -n csa python==3.8\n",
    "# !conda activate csa\n",
    "# !pip install ipykernel\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll have to import the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing functions to scrape web pages\n",
    "Unfortunately, wikipedia does not have a standardized format of publishing the pages. Depending on the year, they use another weblink to publish the page. Using if statements, I'll find the wikiurl for each year. After the year 2006, their page syntax changed as well and I've fixed this using a simple string lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                time\n",
      "1888  page not found\n",
      "1889    no 500m race\n",
      "1890    no 500m race\n",
      "1891    no 500m race\n",
      "1892  page not found\n",
      "1893           51,0 \n",
      "1894           50,4 \n",
      "1895           48,2 \n",
      "1896           50,2 \n",
      "1897    no 500m race\n",
      "1898           47,2 \n",
      "1899           50,5 \n",
      "1900           46,4 \n",
      "1901           54,0 \n",
      "1902           47,0 \n",
      "1903           49,4 \n",
      "1904           46,6 \n",
      "1905           49,8 \n",
      "1906           50,8 \n",
      "1907           47,4 \n",
      "1908           44,8 \n",
      "1909           45,6 \n",
      "1910           46,3 \n",
      "1911           46,4 \n",
      "1912           44,2 \n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid distance entered. Please enter a valid race distance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 144\u001b[0m\n\u001b[1;32m    142\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n\u001b[1;32m    143\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1888\u001b[39m, \u001b[39m1913\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     this_result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([scrape(i, distance)], columns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m], index \u001b[39m=\u001b[39m [i])\n\u001b[1;32m    145\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df, this_result])\n\u001b[1;32m    147\u001b[0m df\u001b[39m.\u001b[39mto_csv(path_or_buf \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m../data/\u001b[39m\u001b[39m{\u001b[39;00mdistance\u001b[39m}\u001b[39;00m\u001b[39mm data - test.csv\u001b[39m\u001b[39m\"\u001b[39m, sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m|\u001b[39m\u001b[39m\"\u001b[39m, header \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, index \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [12], line 12\u001b[0m, in \u001b[0;36mscrape\u001b[0;34m(year, distance)\u001b[0m\n\u001b[1;32m     10\u001b[0m     distance \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m10.000m\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid distance entered. Please enter a valid race distance.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# wikipedia has different URL's depending on the year. After 2006, the page structure also changed, so I'll need a different function to scrape.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m year \u001b[39m<\u001b[39m \u001b[39m2006\u001b[39m:\n",
      "\u001b[0;31mException\u001b[0m: Invalid distance entered. Please enter a valid race distance."
     ]
    }
   ],
   "source": [
    "def scrape(year = int, distance = int):\n",
    "    # first, properly format the distance\n",
    "    if distance == 500:\n",
    "        distance = \"500m\"\n",
    "    elif distance == 1500:\n",
    "        distance = \"1500m\"\n",
    "    elif distance == 5000:\n",
    "        distance = \"5000m\"\n",
    "    elif distance == 10000:\n",
    "        distance = \"10.000m\"\n",
    "    else:\n",
    "        raise Exception(\"Invalid distance entered. Please enter a valid race distance.\")\n",
    "\n",
    "    # wikipedia has different URL's depending on the year. After 2006, the page structure also changed, so I'll need a different function to scrape.\n",
    "    if year < 2006:\n",
    "        if year < 1933:\n",
    "            wikiurl = f\"https://nl.wikipedia.org/wiki/Wereldkampioenschap_schaatsen_allround_{year}\"\n",
    "        elif year < 1996:\n",
    "            wikiurl = f\"https://nl.wikipedia.org/wiki/Wereldkampioenschap_schaatsen_allround_mannen_{year}\"\n",
    "        else:\n",
    "            wikiurl = f\"https://nl.wikipedia.org/wiki/Wereldkampioenschappen_schaatsen_allround_{year}\"\n",
    "\n",
    "        # a page should have response code 200, otherwise it's not legal to scrape the page. Let's check that first:\n",
    "        response = requests.get(wikiurl)\n",
    "        if response.status_code != 200:\n",
    "            # print(f\"URL for year {year} cannot be scraped. Repsonse: {response}\")\n",
    "            return \"page not found\"\n",
    "\n",
    "        # table_class=\"wikitable sortable jquery-tablesorter\"\n",
    "\n",
    "        # parse data from the html into a beautifulsoup object\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table',{'class':\"wikitable\"})\n",
    "\n",
    "        # soup to table:\n",
    "        df=pd.read_html(str(table), header = 0)\n",
    "        # convert list to dataframe\n",
    "        df=pd.DataFrame(df[0])\n",
    "\n",
    "        try:\n",
    "            global df1\n",
    "            df1 = df.copy()\n",
    "            column = df[distance]\n",
    "            try:\n",
    "                row = list(column.str.contains(\"(1)\", regex = False))\n",
    "                row_number = row.index(True)\n",
    "                result = df[distance][row_number]\n",
    "            except:\n",
    "                return \"winner not found - please add data by hand\"\n",
    "        except:\n",
    "            return f\"no {distance} race\"\n",
    "        \n",
    "            # finally, some postprocessing. We have more digits available in later years:\n",
    "        if distance == \"500m\":\n",
    "            if year < 1972:\n",
    "                result = result[0:5]\n",
    "            else:\n",
    "                result = result[0:6]\n",
    "        elif distance == \"1500m\":\n",
    "            if year < 1972:\n",
    "                result = result[0:6]\n",
    "            else:\n",
    "                result = result[0:7]\n",
    "        elif distance == \"5000m\":            \n",
    "            if year < 1972:\n",
    "                result = result[0:6]\n",
    "            else:\n",
    "                result = result[0:7]\n",
    "        elif distance == \"10.000m\":\n",
    "            if year < 1972:\n",
    "                result = result[0:7]\n",
    "            else:\n",
    "                result = result[0:8]\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        wikiurl = f\"https://nl.wikipedia.org/wiki/Wereldkampioenschappen_schaatsen_allround_{year}\"\n",
    "\n",
    "        # a page should have response code 200, otherwise it's not legal to scrape the page. Let's check that first:\n",
    "        response = requests.get(wikiurl)\n",
    "        if response.status_code != 200:\n",
    "            # print(f\"URL for year {year} cannot be scraped. Repsonse: {response}\")\n",
    "            return \"page not found\"\n",
    "\n",
    "        # if there's more tables on a page, we'll just ignore the first ones. To make it hard, wikipedia wouldn't be wikipedia if they didn't have a different syntax on every page\n",
    "        html = response.text\n",
    "        if year < 2010:\n",
    "            einduitslag_start = html.index(\"\"\"<span class=\"mw-headline\" id=\"Eindklassement_2\">Eindklassement</span>\"\"\")\n",
    "        else:\n",
    "            einduitslag_start = html.index(\"\"\"<span class=\"mw-headline\" id=\"Klassement\">Klassement</span>\"\"\")\n",
    "\n",
    "\n",
    "        html = html[einduitslag_start : ]\n",
    "\n",
    "        # parse data from the html into a beautifulsoup object\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        table = soup.find('table',{'class':\"wikitable\"})\n",
    "\n",
    "        # soup to table:\n",
    "        df=pd.read_html(str(table), header = 0)\n",
    "        # convert list to dataframe\n",
    "        df=pd.DataFrame(df[0])\n",
    "        # print(df)\n",
    "\n",
    "        try:\n",
    "            column = df[distance]\n",
    "            try:\n",
    "                row = list(column.str.contains(\"(1)\", regex = False))\n",
    "                row_number = row.index(True)\n",
    "                result = df[distance][row_number]\n",
    "            except:\n",
    "                return \"winner not found - please add data by hand\"\n",
    "        except:\n",
    "            return f\"no {distance} race\"\n",
    "        \n",
    "            # finally, some postprocessing. We have more digits available in later years, and the number of digits also depend on distance\n",
    "        if distance == \"500m\":\n",
    "            if year < 1972:\n",
    "                result = result[0:5]\n",
    "            else:\n",
    "                result = result[0:6]\n",
    "        elif distance == \"1500m\":\n",
    "            if year < 1972:\n",
    "                result = result[0:6]\n",
    "            else:\n",
    "                result = result[0:7]\n",
    "        elif distance == \"5000m\":            \n",
    "            if year < 1972:\n",
    "                result = result[0:6]\n",
    "            else:\n",
    "                result = result[0:7]\n",
    "        elif distance == \"10.000m\":\n",
    "            if year < 1972:\n",
    "                result = result[0:7]\n",
    "            else:\n",
    "                result = result[0:8]\n",
    "        \n",
    "        return result\n",
    "\n",
    "for distance in (500, 1500, 5000, 10000):\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(1888, 1913):\n",
    "        this_result = pd.DataFrame([scrape(i, distance)], columns = ['time'], index = [i])\n",
    "        df = pd.concat([df, this_result])\n",
    "    \n",
    "    df.to_csv(path_or_buf = f\"../data/{distance}m data - test.csv\", sep = \"|\", header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally secure the data by saving it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf = \"10.000m data.csv\", sep = \"|\", header = True, index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('csa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5696f471513769a7bf5b6bfff044cbfdc0a4f2f7fec4320161984df90daa202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
